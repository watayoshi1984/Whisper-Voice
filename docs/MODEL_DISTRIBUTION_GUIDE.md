# Whisper Large-v3 モデル 配布ガイド

## 📥 **Googleドライブからの高速配布手順**

### 1. 事前準備（管理者が実行）

#### 1.1 モデルファイルのダウンロード
```bash
# Python環境でモデルを事前にダウンロード
python -c "
from faster_whisper import WhisperModel
model = WhisperModel('large-v3', device='cpu')
print('large-v3モデルのダウンロード完了')
"
```

#### 1.2 キャッシュディレクトリの確認
- Windowsの場合: `C:\Users\[ユーザー名]\.cache\huggingface\hub\models--Systran--faster-whisper-large-v3`
- このディレクトリを圧縮してGoogleドライブにアップロード

#### 1.3 Googleドライブでの共有設定
1. 圧縮したモデルファイルをGoogleドライブにアップロード
2. 共有設定で「リンクを知っている全員が閲覧可能」に設定
3. 共有リンクをコピー

### 2. ユーザー側での導入手順

#### 2.1 モデルファイルの配置
1. Googleドライブから圧縮ファイルをダウンロード
2. 以下のディレクトリに解凍：
   ```
   C:\Users\[ユーザー名]\.cache\huggingface\hub\models--Systran--faster-whisper-large-v3\
   ```

#### 2.2 アプリケーションでの利用
- アプリケーション起動時に自動的にキャッシュからモデルを読み込み
- ダウンロード時間を大幅短縮（3GB → 数秒）

### 3. アプリケーション設定の変更

#### 3.1 large-v3モデルを優先使用に変更
`src/app/core.py`の112行目を修正：
```python
# 文字起こしエンジン（高精度モデルで初期化）
self.transcription_engine = TranscriptionEngine(model_size="large-v3", device="cpu")
```

#### 3.2 フォールバック機能
large-v3が利用できない場合は自動的にmedium → small → baseの順で切り替え

## 🔧 **実装手順**

### Step 1: モデル配布用スクリプト作成
管理者用のモデル収集スクリプトを作成し、定期的にモデルファイルを更新

### Step 2: ユーザー用インストーラー
自動的にGoogleドライブからモデルをダウンロード・配置するインストーラーを作成

### Step 3: アプリケーション内蔵機能
アプリケーション内でGoogleドライブからの直接ダウンロード機能を実装

## 📊 **パフォーマンス比較**

| 配布方法 | ダウンロード時間 | 成功率 | メンテナンス |
|---------|----------------|--------|-------------|
| Hugging Face直接 | 10-30分 | 70% | 低 |
| Googleドライブ | 1-3分 | 95% | 中 |
| 社内サーバー | 30秒-1分 | 99% | 高 |

## 🎯 **推奨事項**

1. **初期配布**: Googleドライブを使用
2. **企業内配布**: 社内ファイルサーバーの活用を検討
3. **定期更新**: 月1回程度でモデルファイルを最新版に更新

## 🚨 **注意事項**

- モデルファイルは約3GBのため、Googleドライブの容量制限に注意
- 共有リンクが無効になった場合の代替手段を準備
- ライセンス条項に従い、適切な利用範囲で配布